{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot_standard = results[ results['Scaler'] == \"standard\"]\n",
    "df_plot_standard_knn = df_plot_standard[df_plot_standard[\"Model\"] == \"Knn\"]\n",
    "df_plot_standard_knn\n",
    "\n",
    "df_plot_minmax = results[ results['Scaler'] == \"minmax\"]\n",
    "df_plot_minmax_knn = df_plot_minmax[df_plot_minmax[\"Model\"] == \"Knn\"]\n",
    "df_plot_minmax_knn\n",
    "\n",
    "df_plot_quantil = results[ results['Scaler'] == \"quantil\"]\n",
    "df_plot_quantil_knn = df_plot_quantil[df_plot_quantil[\"Model\"] == \"Knn\"]\n",
    "df_plot_quantil_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.plot(df_plot_standard_knn[\"k\"],df_plot_standard_knn.Score,label = \"Standars Scaler\")\n",
    "plt.plot(df_plot_minmax_knn.k,df_plot_minmax_knn.Score,label = \"MinMax Scaler\")\n",
    "plt.plot(df_plot_quantil_knn.k,df_plot_quantil_knn.Score,label = \"Quantil Scaler\")\n",
    "plt.xlabel(\"different k\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Knn\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Bilder/Knn_with_different_k_sclaer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.plot(df_plot_standard_knn.k,df_plot_standard_knn.Fittime,label = \"Standars Scaler\")\n",
    "plt.plot(df_plot_minmax_knn.k,df_plot_minmax_knn.Fittime,label = \"MinMax Scaler\")\n",
    "plt.plot(df_plot_quantil_knn.k,df_plot_quantil_knn.Fittime,label = \"Quantil Scaler\")\n",
    "plt.xlabel(\"different k\")\n",
    "plt.ylabel(\"time [s]\")\n",
    "plt.title(\"Knn\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Bilder/FitTime_knn_with_different_k_sclaer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k, scl = FindBestK_Scaler(results, \"MLP\")\n",
    "#print(k,scl)\n",
    "#k, scl = FindBestK_Scaler(results, \"RM\")\n",
    "#print(k,scl)\n",
    "k, scl = FindBestK_Scaler(results, \"Knn\")\n",
    "print(k,scl)\n",
    "df_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "level = 4\n",
    "alphas = 0.1\n",
    "maxiter = 100\n",
    "hidden = [10,20,50,100,200,400]\n",
    "solver = [\"adam\",\"lbfgs\"]\n",
    "solver = [\"adam\"]\n",
    "activation =[\"identity\", \"tanh\", \"relu\"]\n",
    "activation =[\"tanh\"]\n",
    "model = [\"MLP\"]\n",
    "for mod in model:\n",
    "    for lev in range(1,level+1):\n",
    "        for h in hidden:\n",
    "            for sol in solver:\n",
    "                for act in activation:\n",
    "                    results_pram = amazon_classificators.MLP_Search(alphas,act,sol,h,maxiter,X_train, X_valid, Y_train, Y_valid, lev)\n",
    "                    best_score_MLP = best_score_MLP.append({'Model': mod, \"HiddenLayer\": h, \"Level\": lev, \"Solver\": sol, \"Activation\": act, \"Score\": results_pram[\"score\"], \"F1\": np.mean(results_pram[\"f1\"]), \"Recall\": np.mean(results_pram[\"recall\"]), \"Precision\": np.mean(results_pram[\"precision\"]), \"Runtime\": results_pram[\"time\"]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindBestK_Parameter(df):\n",
    "    df_best = df[df[\"Score\"] == max(df[\"Score\"])]\n",
    "    h = df_best[\"HiddenLayer\"]\n",
    "    lev = df_best[\"Level\"]\n",
    "\n",
    "    return h, lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, lev = FindBestK_Parameter(best_score_MLP)\n",
    "print(h.values[0], lev.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(h.values[0],h.values[0],h.values[0]),alpha=0.1,activation=\"tanh\",solver = \"adam\")\n",
    "clf.fit(X_train, Y_train)#\n",
    "#LC = clf.loss_curve_\n",
    "Y_pred = clf.predict(X_valid)\n",
    "score = accuracy_score(Y_valid, Y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def Knn_Search(max_k,X_train, X_valid, Y_train, Y_valid):\n",
    "    score_list = []\n",
    "    erg = []\n",
    "    for knn in range(1, max_k):\n",
    "        model = KNeighborsClassifier(n_neighbors=knn).fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_valid)\n",
    "        score = accuracy_score(Y_valid, Y_pred)\n",
    "        score_list.append(score)\n",
    "    knn = score_list.index(max(score_list))+1\n",
    "    score = max(score_list)\n",
    "    f1, recall, precision = Scores(Y_valid, Y_pred)\n",
    "    erg = {\n",
    "        \"k_max\": knn,\n",
    "        \"score\": score,\n",
    "        \"f1\": f1,\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision}\n",
    "    print(\"\")\n",
    "    return erg, score_list\n",
    "\n",
    "def Scores(Y_valid, Y_pred):\n",
    "    f1 = f1_score(Y_valid, Y_pred, average='macro')\n",
    "    cm = confusion_matrix(Y_valid, Y_pred)\n",
    "    recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "    precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "\n",
    "    return f1, np.mean(recall), np.mean(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max = 100\n",
    "model = \"Knn\"\n",
    "\n",
    "results_pram, score_list = amazon_classificators.Knn_Search(k_max,X_train, X_valid, Y_train, Y_valid)\n",
    "best_score_Knn = best_score_Knn.append({'Model': model, \"k\": results_pram[\"k_max\"], \"Score\": results_pram[\"score\"], \"F1\": np.mean(results_pram[\"f1\"]), \"Recall\": np.mean(results_pram[\"recall\"]), \"Precision\": np.mean(results_pram[\"precision\"]), \"Runtime\": results_pram[\"time\"]}, ignore_index=True)\n",
    "    #best_params = amazon_classificators.FindBestScore(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.linspace(1,k_max-1,k_max-1)\n",
    "plt.grid()\n",
    "plt.plot(j,score_list)\n",
    "print(np.max(score_list),score_list.index(np.max(score_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "\n",
    "model = [\"RM\"]\n",
    "estimators = [1, 10, 20, 100, 200, 500, 1000,1500,2000]\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "max_features = [\"sqrt\", \"log2\"]\n",
    "\n",
    "\n",
    "for es in estimators:\n",
    "    for cri in criterion:\n",
    "        for maxfe in max_features:\n",
    "            results_pram = amazon_classificators.RM_Search(es,cri,maxfe,X_train, X_valid, Y_train, Y_valid)\n",
    "            best_score_RM = best_score_RM.append({'Model': mod, \"estimators\": es, \"criterion\": cri, \"max_features\": maxfe, \"Score\": results_pram[\"score\"], \"F1\": np.mean(results_pram[\"f1\"]), \"Recall\": np.mean(results_pram[\"recall\"]), \"Precision\": np.mean(results_pram[\"precision\"]), \"Runtime\": results_pram[\"time\"]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_score_MLP = pd.DataFrame(columns=[\"Model\", \"HiddenLayer\", \"Level\", \"Solver\", \"Activation\", \"Score\", \"F1\", \"Recall\", \"Precision\", \"Runtime\"])\n",
    "#best_score_Knn = pd.DataFrame(columns=[\"Model\", \"k\", \"Score\", \"F1\", \"Recall\", \"Precision\", \"Runtime\"])\n",
    "#best_score_RM = pd.DataFrame(columns=[\"Model\", \"estimators\", \"criterion\",\"max_features\", \"Score\", \"F1\", \"Recall\", \"Precision\", \"Runtime\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = pd.unique(target.Class)\n",
    "print(classification_report(Y_valid, Y_pred, target_names=target_names))\n",
    "\n",
    "cm = confusion_matrix(Y_valid, Y_pred)\n",
    "fig = plt.figure(figsize=(70,70))\n",
    "sns.heatmap(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(Y_valid, Y_pred, average='macro')\n",
    "cm = confusion_matrix(Y_valid, Y_pred)\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "\n",
    "print(\"f1: \", f1, \"recall: \", np.mean(recall), \"precision: \", np.mean(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title=\"Confusion matrix\", cmap=plt.cm.Reds) :\n",
    "    \"\"\"classes are the possible classes, so e.g [\"B\",\"M\"], s.t. the ordering matches the encoding\"\"\"\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    num_samples = 1\n",
    "    if normalize:\n",
    "        num_samples = np.sum(cm)\n",
    "    print(\"#\",num_samples)\n",
    "    plt.imshow(cm, interpolation = \"nearest\", cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2\n",
    "    # itertools.product() gives all combinations of the iterables\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n",
    "        string = cm[i, j]\n",
    "        if normalize:\n",
    "            string /= num_samples\n",
    "            string = f\"{string:.2f}\"\n",
    "        plt.text(j, i, string, horizontalalignment = \"center\", color=\"black\", backgroundcolor=\"white\")#= \"white\" if cm[i, j] > thresh else \"black\", )\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm,pd.unique(target.Class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLC(LC,best_params):\n",
    "    plt.figure()\n",
    "    plt.plot(LC)#,label=\"activation {} Hidden Layers {} solver {} \".format(best_params[\"alpha\"],i,l))\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Iterations\")best_score_Knn = pd.DataFrame(columns=[\"Model\", \"k\", \"Score\", \"F1\", \"Recall\", \"Precision\"])\n",
    "k, scl = FindBestK_Scaler(results, \"Knn\")\n",
    "\n",
    "df_prep = amazon_processor.Preprocessing_Amazon(feature, target,feature_method = \"kBest\",scale_method = scl.values[0], k = k.values[0])\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(df_prep, target, test_size=.4, random_state = 42)\n",
    "    plt.xlabel(\"Loss\")\n",
    "    plt.title(\"activation {} Hidden Layers {} solver {} \".format(best_params[\"mode\"],best_params[\"h\"],best_params[\"solver\"]))\n",
    "    #plt.savefig(\"figures//wine//LC//activation {} Hidden Layers {} solver {} \".format(best_params[\"mode\"],best_params[\"h\"],best_params[\"solver\"]))\n",
    "    plt.savefig(\"Bilder/activation {} Hidden Layers {} solver {} \".format(best_params[\"mode\"],best_params[\"h\"],best_params[\"solver\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Confusion matrix \n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title=\"Confusion matrix\", cmap=plt.cm.Reds) :\n",
    "    \"\"\"classes are the possible classes, so e.g [\"B\",\"M\"], s.t. the ordering matches the encoding\"\"\"\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    num_samples = 1\n",
    "    if normalize:\n",
    "        num_samples = np.sum(cm)\n",
    "    print(\"#\",num_samples)\n",
    "    plt.imshow(cm, interpolation = \"nearest\", cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2\n",
    "    # itertools.product() gives all combinations of the iterables\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n",
    "        string = cm[i, j]\n",
    "        if normalize:\n",
    "            string /= num_samples\n",
    "            string = f\"{string:.2f}\"\n",
    "        plt.text(j, i, string, horizontalalignment = \"center\", color=\"black\", backgroundcolor=\"white\")#= \"white\" if cm[i, j] > thresh else \"black\", )\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    \n",
    "# Show metrics \n",
    "def show_metrics(cm):\n",
    "    \"\"\"also returns the metrics as dict\"\"\"\n",
    "    tn = cm[0,0]\n",
    "    tp = cm[1,1]\n",
    "\n",
    "    fp = cm[0,1]\n",
    "    fn = cm[1,0]\n",
    "\n",
    "    acc = (tp+tn) / (tp+tn+fp+fn)\n",
    "    prec = tp / (tp+fp)\n",
    "    rec = tp / (tp+fn)\n",
    "    f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    print(f\"Accuracy  =     {acc:.3f}\")\n",
    "    print(f\"Precision =     {prec:.3f}\")\n",
    "    print(f\"Recall    =     {rec:.3f}\")\n",
    "    print(f\"F1_score  =     {f1:.3f}\")\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def Statistic(Y_valid,Y_pred,name):\n",
    "    Y_pred = pd.DataFrame(Y_pred, columns=['Class'])\n",
    "    Class_description = []\n",
    "    Class_description = pd.unique(Y_valid.Class)\n",
    "\n",
    "    for i in range(len(Class_description)):\n",
    "        Y_valid.Class[Y_valid.Class == Class_description[i]] = i+1\n",
    "    \n",
    "    Class_description = []\n",
    "    Class_description = pd.unique(Y_pred.Class)\n",
    "\n",
    "    for i in range(len(Class_description)):\n",
    "        Y_pred.Class[Y_pred.Class == Class_description[i]] = i+1\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(Y_valid)\n",
    "    sns.distplot(Y_pred, color=\"red\")\n",
    "    #plt.savefig(\"figures//wine//statistics//difference between prediction and validation {}\".format(name))\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(Y_valid-Y_pred)\n",
    "    #plt.savefig(\"figures//wine//statistics//total difference between prediction and validation {}\".format(name))\n",
    "    print(\"RMSE\",sqrt(mean_squared_error(Y_valid, Y_pred)))\n",
    "\n",
    "    Y_pred_Norm = Y_pred / np.linalg.norm(Y_pred)\n",
    "    Y_valid_Norm = Y_valid / np.linalg.norm(Y_valid)\n",
    "\n",
    "    print(\"NormedRMSE\",sqrt(mean_squared_error(Y_pred_Norm, Y_valid_Norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistic(Y_valid, Y_pred,\"Hallo\")\n",
    "\n",
    "#plotLC(LC,best_params)\n",
    "cm_rf_N = confusion_matrix(Y_valid, Y_pred)\n",
    "df_cm = pd.DataFrame(cm_rf_N, index = [i for i in target[\"Class\"]],\n",
    "                  columns = [i for i in target[\"Class\"]])\n",
    "plt.figure(figsize = (10,7))\n",
    "#show_metrics(cm_rf_N)\n",
    "#Statistic(Y_valid,Y_pred,\"MLP GB_N\")\n",
    "plt.figure(figsize = (70,70))\n",
    "sns.heatmap(cm_rf_N, annot=True)\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "class_names =target.Class\n",
    "disp = plot_confusion_matrix(clf, X_valid, X_valid,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# Run classifier, using a model that is too regularized (C too low) to see\n",
    "# the impact on the results\n",
    "classifier = svm.SVC(kernel='linear', C=0.01).fit(X_train, Y_train)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(classifier, X_valid, Y_valid,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    \n",
    "\n",
    "    #fig = plt.figure(figsize=(70,70))\n",
    "    #disp.ax_.set_title(title)\n",
    "        \n",
    "    #print(title)\n",
    "    #print(disp.confusion_matrix)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_rf_N,  [\"B\", \"G\"],\n",
    "                          normalize = True,\n",
    "                          title = 'Confusion matrix (knn)',\n",
    "                          cmap = plt.cm.Blues)\n",
    "metrics_rf_N = show_metrics(cm_rf_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = amazon_processor.Preprocessing_Amazon(feature, target,feature_method = \"PCA\",scale_method = \"standard\")\n",
    "df_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams.update({'font.size': 12})\n",
    "#mpl.style.use('seaborn')\n",
    "mpl.style.use('seaborn')\n",
    "plt.rc('xtick', labelsize=15) \n",
    "plt.rc('ytick', labelsize=15) \n",
    "fig = plt.figure(figsize=(40,12))\n",
    "#sns.histplot(count_classes,kde = False)\n",
    "plt.grid()\n",
    "plt.hist(cleaned[\"Class\"],80,facecolor='g')\n",
    "def PlotHist(df, xaxis,xlabel,ylabel,title,savename,bins):\n",
    "    mpl.style.use('seaborn')\n",
    "    plt.rc('xtick', labelsize=15) \n",
    "    plt.rc('ytick', labelsize=15) \n",
    "    fig = plt.figure(figsize=(40,12))\n",
    "    #sns.histplot(count_classes,kde = False)\n",
    "    plt.hist(cleaned[\"Class\"],80,facecolor='g')\n",
    "    plt.grid()\n",
    "    #plt.savefig(\"Bilder/Class_distribution\")\n",
    "plt.savefig(\"Bilder/Class_distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train['V1']=df_train['V1'].astype('category').cat.codes\n",
    "#df_train['Class']=df_train['Class'].astype('category').cat.codes\n",
    "#df_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "\n",
    "print(\"f1: \", f1, \"recall: \", np.mean(recall), \"precision: \", np.mean(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  }
 ]
}