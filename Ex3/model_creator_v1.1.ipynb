{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code taken from https://medium.com/@aungkyawmyint_26195/multi-layer-perceptron-mnist-pytorch-463f795b897a\n",
    "\n",
    "New things:\n",
    "* Used a binary transformer with thresholding\n",
    "* added functionality for switching between different artchitecutres\n",
    "\n",
    "Other\n",
    "* replaced some magic numbers with config variables\n",
    "* Added variable to switch between architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from config import *\n",
    "\n",
    "\n",
    "from common import plot_batch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.mnist_binary_conf import * # load configuration file for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the training and testing datasets\n",
    "train_data = datasets.MNIST(root = \"data\", train = True, download = True, transform = transform)\n",
    "test_data = datasets.MNIST(root = \"data\", train = False, download = True, transform = transform)\n",
    "\n",
    "assert sample_size >= 0  # Error: Invalid TRAIN_SIZE\n",
    "assert sample_size <= len(train_data) # Error: Invalid TRAIN_SIZE\n",
    "\n",
    "# set number of subsamples \n",
    "if sample_size == 0: num_train = len(train_data)\n",
    "else: num_train = sample_size\n",
    "\n",
    "# obtain training indices that will be used for validation \n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_index, valid_index = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "valid_sampler = SubsetRandomSampler(valid_index)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = train_sampler, num_workers = num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = valid_sampler, num_workers = num_workers)\n",
    "test_loader =  torch.utils.data.DataLoader(test_data,  batch_size = batch_size, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize a batch of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "plot_batch(images, labels, label_color=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()    \n",
    "        layers(self) # defined in the loaded conf file\n",
    "\n",
    "    def forward(self, x):   \n",
    "        return forward(self, x) # defined in loaded conf file\n",
    "\n",
    "class Net_Relu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_Relu,self).__init__()    \n",
    "        layers(self) # defined in the loaded conf file\n",
    "\n",
    "    def forward(self, x):   \n",
    "        return forward_relu(self, x) # defined in loaded conf file\n",
    "    \n",
    "model_file_name = file_name_relu\n",
    "# initialize the NN\n",
    "#model = Net()\n",
    "model = Net_Relu()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_TRAIN = True\n",
    "#ENABLE_TRAIN = False # So you can switch this off, if you want to rerun the entire notebook\n",
    "model_file_name = MODEL_DIR + \"model_relu_peter.pth\"# if you want to override where the model params are saved to!\n",
    "if ENABLE_TRAIN:\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "    for epoch in range(n_epochs):\n",
    "        # monitor losses\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for data,label in train_loader:\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output,label)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()  # prep model for evaluation\n",
    "        for data,label in valid_loader:\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output,label)\n",
    "            # update running validation loss \n",
    "            valid_loss = loss.item() * data.size(0)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "\n",
    "        print(\"Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\".format(\n",
    "            epoch+1, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(\"Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    print(\"### --------- TRAINING DONE --------- ###\")\n",
    "else:\n",
    "    print(\"### --------- DID NOT TRAIN --------- ###\")\n",
    "    print(\"Maybe you forgot to enable training for this notebook?\")\n",
    "    print(\"Check the top of this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a crypten model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_TRAIN = True\n",
    "ENABLE_TRAIN = False # So you can switch this off, if you want to rerun the entire notebook\n",
    "#model_file_name = MODEL_DIR + \"model_relu_peter.pth\"# if you want to override where the model params are saved to!\n",
    "\n",
    "\n",
    "#criterion = crypten.nn.CrossEntropyLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if ENABLE_TRAIN:\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "    for epoch in range(n_epochs):\n",
    "        # monitor losses\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for data,label in train_loader:\n",
    "            # clear the gradients of all optimized variables\n",
    "            #optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            #display(output)\n",
    "            #print(output.shape, label.shape)\n",
    "            loss = criterion(output, label)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            #optimizer.step()\n",
    "            model.update_parameters(learning_rate)\n",
    "            # update running training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()  # prep model for evaluation\n",
    "        for data,label in valid_loader:\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output,label)\n",
    "            # update running validation loss \n",
    "            valid_loss = loss.item() * data.size(0)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "\n",
    "        print(\"Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\".format(\n",
    "            epoch+1, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(\"Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    print(\"### --------- TRAINING DONE --------- ###\")\n",
    "else:\n",
    "    print(\"### --------- DID NOT TRAIN --------- ###\")\n",
    "    print(\"Maybe you forgot to enable training for this notebook?\")\n",
    "    print(\"Check the top of this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model with Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error when loading\n",
    "\n",
    "I got this error when loading Marios model...\n",
    "\n",
    "```bash\n",
    "RuntimeError: version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at /pytorch/caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at /pytorch/caffe2/serialize/inline_container.cc:132)\n",
    "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7f13cbfd0193 in /home/peter/.local/share/virtualenvs/Ex3-pG_9TV2D/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
    "frame #1: caffe2::serialize::PyTorchStreamReader::init() + 0x1f5b (0x7f13cf1589eb in /home/peter/.local/share/virtualenvs/Ex3-pG_9TV2D/lib/python3.7/site-packages/torch/lib/libtorch.so)\n",
    "frame #2: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::string const&) + 0x64 (0x7f13cf159c04 in /home/peter/.local/share/virtualenvs/Ex3-pG_9TV2D/lib/python3.7/site-packages/torch/lib/libtorch.so)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the trained Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single party/standard variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "# NUM_CLASSES = 10\n",
    "\n",
    "test_loss = 0.0\n",
    "class_correct = [0] * NUM_CLASSES\n",
    "class_total = [0] * NUM_CLASSES \n",
    "\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "for data, target in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "print(f\"Test Loss: {test_loss:.6}\\n\")\n",
    "# Print accuracy per class\n",
    "for i in range(NUM_CLASSES):\n",
    "    if class_total[i] > 0:\n",
    "        print(f\"Test Accuracy of {i:5}: \" \n",
    "              f\"{100 * class_correct[i] / class_total[i]:3.0f}% \"\n",
    "              f\"({np.sum(class_correct[i]):4} / {np.sum(class_total[i]):4} )\")\n",
    "    else:\n",
    "        print(f\"Test Accuracy of {classes[i]}: N/A (no training examples)\")\n",
    "# Print overall accuracy\n",
    "print(f\"\\nTest Accuracy (Overall): {100. * np.sum(class_correct) / np.sum(class_total):3.0f}% \"\n",
    "      f\"( {np.sum(class_correct)} / {np.sum(class_total)} )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPC variant\n",
    "\n",
    "Ideally, we should see the same test results here as well...it's just going to take much longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "\n",
    "assert sys.version_info[0] == 3 and sys.version_info[1] == 7, \"python 3.7 is required!\"\n",
    "\n",
    "print(f\"Okay, good! You have: {sys.version_info[:3]}\")\n",
    "# Now we can init crypten!\n",
    "crypten.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import crypten.communicator as comm # the communicator is similar to the MPI communicator for example\n",
    "from crypten import mpc\n",
    "from multiprocessing import Barrier\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "from ex3_lib.dir_setup import POSSIBLE_PARTICIPANTS, check_and_mkdir\n",
    "\n",
    "import warnings; \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "check_and_mkdir(pathlib.Path(\"./log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_legacy_config():\n",
    "    if \"MNIST_IMG_HWIDTH\" in locals():\n",
    "        if \"MNIST_IMG_HEIGHT\" in locals():\n",
    "            IMG_HEIGHT = MNIST_IMG_HEIGHT\n",
    "        else:\n",
    "            IMG_HEIGHT = 28\n",
    "    if \"MNIST_IMG_HWIDTH\" in locals():\n",
    "        if \"MNIST_IMG_HWIDTH\" in locals():\n",
    "            IMG_WIDTH = MNIST_IMG_HWIDTH\n",
    "        else:\n",
    "            IMG_WIDTH = 28\n",
    "    if \"IMAGE_TYPE\" not in locals():\n",
    "        IMAGE_TYPE = \"grayscale\"\n",
    "        if \"NUM_CHANNELS\" not in locals():\n",
    "            NUM_CHANNELS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEGACY\n",
    "convert_legacy_config()\n",
    "dummy_image = torch.empty([1, NUM_CHANNELS, IMG_WIDTH, IMG_HEIGHT]) # is that the right way around? :D\n",
    "model_enc = crypten.nn.from_pytorch(model, dummy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 30/500 [00:44<12:19,  1.57s/it]"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "# NUM_CLASSES = 10\n",
    "num_participants = 2\n",
    "participants = POSSIBLE_PARTICIPANTS[:num_participants]\n",
    "torch.set_num_threads(1) #\n",
    "\n",
    "assert len(participants) == num_participants # checking for shenanigans\n",
    "\n",
    "class_correct = [0] * NUM_CLASSES\n",
    "class_total = [0] * NUM_CLASSES \n",
    "runtime = 0\n",
    "\n",
    "convert_legacy_config() # LEGACY\n",
    "dummy_image = torch.empty([1, NUM_CHANNELS, IMG_WIDTH, IMG_HEIGHT]) # is that the right way around? :D\n",
    "model_mpc = crypten.nn.from_pytorch(model, dummy_image)\n",
    "\n",
    "\n",
    "before_test = Barrier(num_participants)\n",
    "after_test = Barrier(num_participants)\n",
    "#criterion = crypten.nn.CrossEntropyLoss()\n",
    "criterion = crypten.nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "@mpc.run_multiprocess(world_size=num_participants)\n",
    "def test_model_mpc():\n",
    "    pid = comm.get().get_rank()\n",
    "    ws = comm.get().world_size\n",
    "    name = participants[pid]\n",
    "    if pid == 0:\n",
    "        print(f\"Hello from the main process (rank#{pid} of {ws})!\")\n",
    "        print(f\"My name is {name}.\")\n",
    "        print(f\"My colleagues today are: \")\n",
    "        print(participants)\n",
    "        \n",
    "    model_mpc.encrypt(src=0)\n",
    "        \n",
    "    before_test.wait()  \n",
    "    if pid == 0:\n",
    "        print(\"Gonna evaluate now...\")\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    model_mpc.eval() # prep model for evaluation\n",
    "    if pid==0:\n",
    "        start = time()\n",
    "    for data, target in tqdm(test_loader, position=0): #, desc=f\"{name}\"):\n",
    "        data_enc = crypten.cryptensor(data, src=pid)\n",
    "        target_enc = crypten.cryptensor(target, src=pid)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_mpc(data_enc)\n",
    "        # decrypt output\n",
    "        #output = output.get_plain_text()\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.argmax(dim=1, one_hot=False)\n",
    "        # calculate the loss\n",
    "        if pid == 0:\n",
    "            if pred.shape != target.shape:\n",
    "                print((pred.shape, target_enc.shape))\n",
    "        loss = criterion(pred, target_enc).get_plain_text()\n",
    "        # update test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        # compare predictions to true label\n",
    "        #pred = pred.get_plain_text()\n",
    "        # decrypt predictions\n",
    "        pred = pred.get_plain_text()\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(len(target)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "    if pid==0:\n",
    "        stop = time()\n",
    "        runtime = stop - start\n",
    "        \n",
    "    if pid == 0:\n",
    "        print(\"Done evaluating...\")\n",
    "    \n",
    "    after_test.wait()\n",
    "    \n",
    "    if pid == 0:\n",
    "        print(\"Ouputing information...\")\n",
    "\n",
    "    # calculate and print avg test loss\n",
    "    test_loss = test_loss/len(test_loader.sampler)\n",
    "    if pid == 0:\n",
    "        print(f\"Test Loss: {test_loss:.6}\\n\")\n",
    "        # Print accuracy per class\n",
    "        for i in range(NUM_CLASSES):\n",
    "            if class_total[i] > 0:\n",
    "                print(f\"Test Accuracy of {i:5}: \"\n",
    "                      f\"{100 * class_correct[i] / class_total[i]:3.0f}% \"\n",
    "                      f\"({np.sum(class_correct[i]):4} / {np.sum(class_total[i]):4} )\")\n",
    "            else:\n",
    "                print(f\"Test Accuracy of {classes[i]}: N/A (no training examples)\")\n",
    "        # Print overall accuracy\n",
    "        print(f\"\\nTest Accuracy (Overall): {100. * np.sum(class_correct) / np.sum(class_total):3.0f}% \"\n",
    "              f\"( {np.sum(class_correct)} / {np.sum(class_total)} )\")\n",
    "    \n",
    "    # Gather log\n",
    "    LOG_STR = f\"Rank: {pid}\\nWorld_Size: {ws}\\n\\n\"\n",
    "    LOG_STR += f\"Test Loss: {test_loss:.6}\\n\"\n",
    "    LOG_STR += \"\\n\"\n",
    "    for i in range(NUM_CLASSES):\n",
    "        if class_total[i] > 0:\n",
    "            LOG_STR += f\"Test Accuracy of {i:5}: \" \\\n",
    "                  f\"{100 * class_correct[i] / class_total[i]:3.0f}% \" \\\n",
    "                  f\"({np.sum(class_correct[i]):4} / {np.sum(class_total[i]):4} )\"\n",
    "            LOG_STR += \"\\n\"\n",
    "        else:\n",
    "            LOG_STR += f\"Test Accuracy of {classes[i]}: N/A (no training examples)\"\n",
    "            LOG_STR += \"\\n\"\n",
    "    LOG_STR += f\"\\nTest Accuracy (Overall): {100. * np.sum(class_correct) / np.sum(class_total):3.0f}% \" + \\\n",
    "          f\"( {np.sum(class_correct)} / {np.sum(class_total)} )\"\n",
    "    with open(f\"log/test_log_rank{pid}\", \"w\") as f:\n",
    "        f.write(LOG_STR)\n",
    "        \n",
    "test_model_mpc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Sample Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "# get sample outputs\n",
    "output = model(images)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds = torch.max(output, 1)\n",
    "\n",
    "plot_batch(images, labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
