{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "future-indiana",
   "metadata": {},
   "source": [
    "# Let's put it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import crypten\n",
    "\n",
    "assert sys.version_info[0] == 3 and sys.version_info[1] == 7, \"python 3.7 is required!\"\n",
    "\n",
    "print(f\"Okay, good! You have: {sys.version_info[:3]}\")\n",
    "# Now we can init crypten!\n",
    "crypten.init()\n",
    "\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parent folders into path\n",
    "import os,sys,inspect\n",
    "import pathlib\n",
    "import shutil\n",
    "import crypten.communicator as mpc_comm # the communicator is similar to the MPI communicator for example\n",
    "from crypten import mpc\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "# Import some config variables\n",
    "from config import PETER_ROOT, DATA_DIR, MNIST_SIZE\n",
    "\n",
    "# Load a pytorch net\n",
    "from ZeNet.nets import *\n",
    "\n",
    "# Plotting\n",
    "from plot_mnist import plot_batch, plot_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_mkdir(directory:pathlib.Path):\n",
    "    if not directory.exists():\n",
    "        print(\"TMP_DIR created\")\n",
    "        directory.mkdir()\n",
    "\n",
    "def rm_dir(directory:pathlib.Path):\n",
    "    shutil.rmtree(directory)\n",
    "    \n",
    "\n",
    "def get_filenames(directory:pathlib.Path):\n",
    "    # Specify file locations to save each piece of data\n",
    "    filenames = {\n",
    "        \"features\": directory / \"features.pth\",\n",
    "        \"labels\": directory / \"labels.pth\",\n",
    "        \"b_true\": directory / \"b_true.pth\",\n",
    "        \"test_features\": directory / \"test_features.pth\",\n",
    "        \"targets\": directory / \"targets.pth\",\n",
    "        \"w_true\": directory / \"w_true.pth\",\n",
    "    }\n",
    "\n",
    "    for u in participants:\n",
    "        filenames[\"labels_\"+u] = directory / (\"labels_\" + u)\n",
    "        filenames[\"features_\"+u] = directory / (\"features_\" + u)\n",
    "    return filenames\n",
    "\n",
    "def setup(participants, tmp_dir_name=\"./TMP\"):\n",
    "    num_participants = len(participants)\n",
    "    TMP_DIR = pathlib.Path(tmp_dir_name)\n",
    "    print(f\"Our temporary data will land here: {TMP_DIR}\")\n",
    "    check_and_mkdir(TMP_DIR)\n",
    "    filenames = get_filenames(TMP_DIR)\n",
    "    return TMP_DIR, filenames, num_participants \n",
    "\n",
    "POSSIBLE_PARTICIPANTS = (\"alice, bob, clara, daniel, \" + \n",
    "    \"elina, franz, georg, hilda, ilya, julia, karin, luke, \" +\n",
    "    \"martin, nadia, olaf, peter, queenie, rasmus, sarah, tal, \" +\n",
    "    \"ulyana, valerie, walter, xander, ymir, zorro\").split(\", \")\n",
    "len(POSSIBLE_PARTICIPANTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Across\n",
    "ALICE = 0\n",
    "BOB = 1\n",
    "participants = POSSIBLE_PARTICIPANTS[:2]\n",
    "dir_name = \"./TMP_\" + \"train_on_shared_data\"\n",
    "\n",
    "TMP_DIR, filenames, num_participants = setup(participants, tmp_dir_name=dir_name)\n",
    "DATA_DIR = TMP_DIR / \"data\"\n",
    "participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(participants)\n",
    "print(TMP_DIR)\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 1/60\n",
    "train_ratio = 0.75\n",
    "test_ratio = 1 - train_ratio\n",
    "batch_size_train = int((subset * MNIST_SIZE) * train_ratio)\n",
    "batch_size_test = int((subset * MNIST_SIZE) * test_ratio)\n",
    "\n",
    "print(f\"Using train_test ratios: {train_ratio} : {test_ratio}\")\n",
    "print(f\"Train batch size: {batch_size_train}\")\n",
    "print(f\"Test batch size: {batch_size_test}\")\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(DATA_DIR, train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "loader_test = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(DATA_DIR, train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "train_batches = enumerate(loader_train)\n",
    "train_idx, digits = next(train_batches)\n",
    "test_batches = enumerate(loader_test)\n",
    "test_idx, digits_test = next(train_batches)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "plot_digit(digits[0][img_num], digits[1][img_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, frac):\n",
    "    length = len(data[1]) #.shape[0]\n",
    "    split_idx = int(length*frac)\n",
    "    print(f\"Returning: 0 <-1-> {split_idx} <-2->{length}\")\n",
    "    feats_1, labels_1 = data[0][:split_idx], data[1][:split_idx]\n",
    "    feats_2, labels_2 = data[0][split_idx:], data[1][split_idx:]\n",
    "    return (feats_1, labels_1), (feats_2, labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_alice = 0.6\n",
    "frac_bob = 1 - frac_alice\n",
    "\n",
    "# Split data and save\n",
    "dig_alice, dig_bob = split_data(digits, frac_alice)\n",
    "\n",
    "\n",
    "# Save features, labels for Data Labeling example\n",
    "crypten.save(digits[0], filenames[\"features\"])\n",
    "crypten.save(digits[1], filenames[\"labels\"])\n",
    "\n",
    "@mpc.run_multiprocess(world_size=num_participants)\n",
    "def save_all_data():\n",
    "    \n",
    "    print(f\"Hello from {mpc_comm.get().get_rank()}\")\n",
    "    \n",
    "    crypten.save(dig_alice[0], filenames[\"features_alice\"], src=ALICE)\n",
    "    crypten.save(dig_bob[0], filenames[\"features_bob\"], src=BOB)\n",
    "    \n",
    "    # Save split dataset for Dataset Aggregation example\n",
    "    crypten.save(dig_alice[1], filenames[\"labels_alice\"], src=ALICE)\n",
    "    crypten.save(dig_bob[1], filenames[\"labels_bob\"], src=BOB)\n",
    "    \n",
    "    # Save true model weights and biases for Model Hiding example\n",
    "#     crypten.save(w_true, filenames[\"w_true\"], src=ALICE)\n",
    "#     crypten.save(b_true, filenames[\"b_true\"], src=ALICE) \n",
    "    \n",
    "    crypten.save(digits_test[0], filenames[\"test_features\"], src=BOB)\n",
    "    crypten.save(digits_test[1], filenames[\"targets\"], src=BOB)\n",
    "    print(f\"{mpc_comm.get().get_rank()} is done! Signing off...\")\n",
    "    \n",
    "save_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-upset",
   "metadata": {},
   "source": [
    "# Load the data into the respective threads\n",
    "\n",
    "*Note*: We will use the term thread in a sloppy manner, i.e. a thread is synonymous with process or a participant (quite possibly running on a completely different PC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-syndication",
   "metadata": {},
   "source": [
    "## About the mpc.run_multiprocess decorator\n",
    "\n",
    "To execute multi-party computations locally, we provide a `@mpc.run_multiprocess` function decorator, which we developed to execute CrypTen code from a single script. CrypTen follows the standard MPI programming model: it runs a separate process for each party, but each process runs an identical (complete) program. Each process has a rank variable to identify itself.\n",
    "\n",
    "[Docs](https://crypten.readthedocs.io/en/latest/mpctensor.html#communicator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-prime",
   "metadata": {},
   "source": [
    "### Scenario: Alice Data -> Bob Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummz = []\n",
    "dummz.append(1)\n",
    "dummz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I mean, really just send the encrypted data to Bob\n",
    "\n",
    "def load_enc_data(X_files, y_files):\n",
    "    # Load images\n",
    "    X = []\n",
    "    for file, rank in X_files:\n",
    "        X.append(crypten.load(file, src=rank))\n",
    "        \n",
    "    # Load labels\n",
    "    y = []\n",
    "    for file, rank in y_files:\n",
    "        y.append(crypten.load(file, src=rank))\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "def load_model(Net, dummy_input, ENC_RANK):\n",
    "    # (1, 1, 28, 28)\n",
    "    private_model = crypten.nn.from_pytorch(Net, dummy_input)\n",
    "    private_model.encrypt(src=ENC_RANK)\n",
    "    return private_model\n",
    "\n",
    "def train_model(model, X, y, epochs=10, learning_rate=0.05):\n",
    "    criterion = crypten.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        print(f\"epoch {epoch} loss: {loss.get_plain_text()}\")\n",
    "        loss.backward()\n",
    "        model.update_parameters(learning_rate)\n",
    "    return model\n",
    "\n",
    "def evaluate_model():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-haven",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mpc.run_multiprocess(world_size=num_participants)\n",
    "def load_data_and_encrypt():\n",
    "    pid = mpc_comm.get().get_rank()\n",
    "    print(f\"pid: {pid}\")\n",
    "    \n",
    "    test = crypten.load(filenames[\"labels_alice\"], src=ALICE)\n",
    "    print(test.get_plain_text())\n",
    "\n",
    "load_data_and_encrypt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.load('/tmp/bob_test_labels.pth').long()\n",
    "count = 100 # For illustration purposes, we'll use only 100 samples for classification\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def encrypt_model_and_data():\n",
    "    # Load pre-trained model to Alice\n",
    "    model = crypten.load('models/tutorial4_alice_model.pth', dummy_model=dummy_model, src=ALICE)\n",
    "    \n",
    "    # Encrypt model from Alice \n",
    "    dummy_input = torch.empty((1, 784))\n",
    "    private_model = crypten.nn.from_pytorch(model, dummy_input)\n",
    "    private_model.encrypt(src=ALICE)\n",
    "    \n",
    "    # Load data to Bob\n",
    "    data_enc = crypten.load('/tmp/bob_test.pth', src=BOB)\n",
    "    data_enc2 = data_enc[:count]\n",
    "    data_flatten = data_enc2.flatten(start_dim=1)\n",
    "\n",
    "    # Classify the encrypted data\n",
    "    private_model.eval()\n",
    "    output_enc = private_model(data_flatten)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    output = output_enc.get_plain_text()\n",
    "    accuracy = compute_accuracy(output, labels[:count])\n",
    "    print(\"\\tAccuracy: {0:.4f}\".format(accuracy.item()))\n",
    "    \n",
    "encrypt_model_and_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-swift",
   "metadata": {},
   "source": [
    "## Running on different machines\n",
    "\n",
    "Tricky: https://github.com/facebookresearch/CrypTen/issues/104\n",
    "\n",
    "Scripts: <https://github.com/facebookresearch/CrypTen/tree/master/scripts>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-jenny",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML_2020)",
   "language": "python",
   "name": "ml_2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
