{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('ML_2020': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "d6f8d2b41179c5e5fc034bbc40427a4395636c4d26988e51920bd3cc303b8725"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "source": [
    "# PyTorch - My Getting Started Notebook\n",
    "\n",
    "I'm mainly following this tutorial: https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "\n",
    "\n",
    "Book about pytorch: https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf\n",
    "\n",
    "Other useful resource: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "Also I'm using a net from a pytorch example: https://github.com/pytorch/examples/blob/master/mnist/main.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperHyper!\n",
    "n_epochs = 3\n",
    "\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "# torch.backends.cudnn.enabled = False # I don't have a nvidia gfx card on my laptop\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Directory setup\n",
    "result_path = pathlib.Path.cwd() / \"results\"\n",
    "data_dir = pathlib.Path.cwd() / \"data\"\n",
    "\n",
    "# Just checking if they already exist,\n",
    "# if not create it.\n",
    "print(result_path)\n",
    "if not result_path.exists():\n",
    "    print(\"Path not found...mkdir\")\n",
    "    result_path.mkdir()\n",
    "else:\n",
    "    print(\"Already exists\")\n",
    "\n",
    "print(data_dir)\n",
    "if not data_dir.exists():\n",
    "    print(\"Path not found...mkdir\")\n",
    "    data_dir.mkdir()\n",
    "else:\n",
    "    print(\"Already exists\")\n"
   ]
  },
  {
   "source": [
    "# Load the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(data_dir, train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(data_dir, train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "  # If he needs to download it, cause it's not already in the data folder, he/she/it would do so and say so below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"type: {type(example_data)}\")\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch(batch, b_range=5):\n",
    "    if isinstance(b_range, tuple):\n",
    "        if len(b_range) ==2:\n",
    "            (start, stop) = b_range\n",
    "            step = 1\n",
    "        elif len(b_range) ==3:\n",
    "            (start, stop, step) = b_range\n",
    "        else:\n",
    "            raise ValueError(\"b_range Should have len==2\")\n",
    "    else:\n",
    "        start, stop = 0, b_range\n",
    "    fig = plt.figure()\n",
    "    num_to_plot = int(np.ceil((stop - start) / step))\n",
    "    print(f\"num_to_plot: {num_to_plot}\")\n",
    "    cols = 4\n",
    "    rows = int(np.ceil(num_to_plot / cols))\n",
    "    print(f\"rows, cols = {rows}, {cols}\")\n",
    "    for i in range(start, stop, step):\n",
    "        subplot_num = int(1 + ((i - start)/step) % num_to_plot)\n",
    "        print(f\"i: {i} --> subplot#: {subplot_num}\")\n",
    "        plt.subplot(rows, cols, subplot_num)\n",
    "        plt.imshow(batch[i][0], cmap='gray', interpolation='none')\n",
    "        plt.title(f\"Label(#{i}): {example_targets[i]}\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch(example_data, (5,20, 2))"
   ]
  },
  {
   "source": [
    "# Define our model\n",
    "\n",
    "see `petenet.py`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some stuff we need for models\n",
    "from ZeNet.nets import Net1, Net2\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn # contain trainable params\n",
    "import torch.nn.functional as F # purely functional"
   ]
  },
  {
   "source": [
    "## Initialize the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net1()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "# If we want to use another device then we'd have to/should sent the network parameters to the device, e.g. for a CUDA GPU `network.cuda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      torch.save(network.state_dict(), result_path/'model.pth')\n",
    "      torch.save(optimizer.state_dict(), result_path/'optimizer.pth')\n",
    "\n",
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "  \n",
    "def train_and_test():\n",
    "  train_losses = []\n",
    "  train_counter = []\n",
    "  test_losses = []\n",
    "  test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "  test()\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()\n",
    "  return train_losses, train_counter, test_losses, test_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses1 = train_losses\n",
    "train_counter1 = train_counter\n",
    "test_losses1 = test_losses\n",
    "test_counter1 = test_counter\n",
    "\n",
    "network1 = network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net1()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "train_losses2, train_counter2, test_losses2, test_counter2 = train_and_test()\n",
    "network2 = network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}